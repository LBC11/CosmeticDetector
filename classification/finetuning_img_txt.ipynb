{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchsummary import summary\n",
    "from typing import Callable, Dict, List, Tuple, Union\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import clip #importing clip model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available:\n",
    "  print('cuda available')\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP 모델과 tokenizer를 로드합니다.\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "tokenizer = clip.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=False) #exploiting CLIP preprocess for dataset transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=False) #exploiting CLIP preprocess for dataset transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(cifar100, [40000, 8000,2000])\n",
    "# print(cifar100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지와 텍스트를 가져와서 임베딩을 생성합니다.\n",
    "image_input = torch.tensor(np.stack([test_set[x][0] for x in range(len(test_set))])).to(device)\n",
    "\n",
    "# test_set에 있는 이미지들의 클래스 정보를 가져옵니다.\n",
    "test_classes = [cifar100.classes[idx] for (_, idx) in test_set]\n",
    "\n",
    "# 클래스 정보를 이용하여 텍스트를 생성합니다.\n",
    "text_input = [f\"a photo of {label}\" for label in test_classes]\n",
    "\n",
    "# 텍스트를 토큰화하여 텐서로 변환합니다.\n",
    "text_input = clip.tokenize(text_input).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "    text_features = model.encode_text(text_input).float()\n",
    "\n",
    "# 이미지와 텍스트 임베딩을 결합합니다.\n",
    "features = torch.cat([image_features, text_features], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0510,  0.0335, -0.1149,  ...,  0.0068,  0.0726,  0.4988],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 1024])\n",
      "torch.Size([2000, 512])\n",
      "torch.Size([2000, 512])\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(image_features.shape)\n",
    "print(image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data : len(test_classes)\n",
      "Num of class : len(set(test_classes))\n"
     ]
    }
   ],
   "source": [
    "print(f'Num of data : len(test_classes)')\n",
    "print(f'Num of class : len(set(test_classes))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size = batch_size, #배치사이즈\n",
    "                                               shuffle = True,\n",
    "                                               num_workers=num_workers\n",
    "                                               )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                               batch_size = batch_size,                                              \n",
    "                                               shuffle = True,\n",
    "                                                num_workers=num_workers\n",
    "                                              )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
