{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HiplefhuIUDd"
   },
   "source": [
    "# **CLIP Explainability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ogYpvQAAH4s",
    "outputId": "9fff6b83-2551-4644-d59b-bc627535ac97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (0.6.1)\n",
      "Requirement already satisfied: ftfy in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/iai/.local/lib/python3.9/site-packages (from ftfy) (0.2.6)\n",
      "Requirement already satisfied: captum in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: matplotlib in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from captum) (3.7.1)\n",
      "Requirement already satisfied: numpy in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from captum) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.6 in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from captum) (1.12.1)\n",
      "Requirement already satisfied: typing_extensions in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from torch>=1.6->captum) (4.6.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from matplotlib->captum) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from matplotlib->captum) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from matplotlib->captum) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/iai/.local/lib/python3.9/site-packages (from matplotlib->captum) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from matplotlib->captum) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from matplotlib->captum) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/iai/.local/lib/python3.9/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from matplotlib->captum) (5.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/iai/anaconda3/envs/jwnew2/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->captum) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/iai/.local/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/hila-chefer/Transformer-MM-Explainability\n",
    "\n",
    "# import os\n",
    "# os.chdir(f'./Transformer-MM-Explainability')\n",
    "\n",
    "!pip install einops\n",
    "!pip install ftfy\n",
    "!pip install captum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "P8sl0DTeHuKx"
   },
   "source": [
    "# **CLIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "or8UETbZAYY3"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CLIP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mCLIP\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclip\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mclip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# import clip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'CLIP'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import CLIP.clip as clip\n",
    "# import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import visualization\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DhG24G9cTpHT"
   },
   "outputs": [],
   "source": [
    "#@title Control context expansion (number of attention layers to consider)\n",
    "#@title Number of layers for image Transformer\n",
    "start_layer =  -1#@param {type:\"number\"}\n",
    "\n",
    "#@title Number of layers for text Transformer\n",
    "start_layer_text =  -1#@param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fWKGyu2YAeSV"
   },
   "outputs": [],
   "source": [
    "def interpret(image, texts, model, device, start_layer=start_layer, start_layer_text=start_layer_text):\n",
    "    batch_size = texts.shape[0]\n",
    "    images = image.repeat(batch_size, 1, 1, 1)\n",
    "    logits_per_image, logits_per_text = model(images, texts)\n",
    "    probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
    "    index = [i for i in range(batch_size)]\n",
    "    one_hot = np.zeros((logits_per_image.shape[0], logits_per_image.shape[1]), dtype=np.float32)\n",
    "    one_hot[torch.arange(logits_per_image.shape[0]), index] = 1\n",
    "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "    one_hot = torch.sum(one_hot.cuda() * logits_per_image)\n",
    "    model.zero_grad()\n",
    "\n",
    "    image_attn_blocks = list(dict(model.visual.transformer.resblocks.named_children()).values())\n",
    "\n",
    "    if start_layer == -1: \n",
    "      # calculate index of last lareyer \n",
    "      start_layer = len(image_attn_blocks) - 1\n",
    "    \n",
    "    num_tokens = image_attn_blocks[0].attn_probs.shape[-1]\n",
    "    R = torch.eye(num_tokens, num_tokens, dtype=image_attn_blocks[0].attn_probs.dtype).to(device)\n",
    "    R = R.unsqueeze(0).expand(batch_size, num_tokens, num_tokens)\n",
    "    for i, blk in enumerate(image_attn_blocks):\n",
    "        if i < start_layer:\n",
    "          continue\n",
    "        grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
    "        cam = blk.attn_probs.detach()\n",
    "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "        cam = grad * cam\n",
    "        cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
    "        cam = cam.clamp(min=0).mean(dim=1)\n",
    "        R = R + torch.bmm(cam, R)\n",
    "    image_relevance = R[:, 0, 1:]\n",
    "\n",
    "    \n",
    "    text_attn_blocks = list(dict(model.transformer.resblocks.named_children()).values())\n",
    "\n",
    "    if start_layer_text == -1: \n",
    "      # calculate index of last layer \n",
    "      start_layer_text = len(text_attn_blocks) - 1\n",
    "\n",
    "    num_tokens = text_attn_blocks[0].attn_probs.shape[-1]\n",
    "    R_text = torch.eye(num_tokens, num_tokens, dtype=text_attn_blocks[0].attn_probs.dtype).to(device)\n",
    "    R_text = R_text.unsqueeze(0).expand(batch_size, num_tokens, num_tokens)\n",
    "    for i, blk in enumerate(text_attn_blocks):\n",
    "        if i < start_layer_text:\n",
    "          continue\n",
    "        grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
    "        cam = blk.attn_probs.detach()\n",
    "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "        cam = grad * cam\n",
    "        cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
    "        cam = cam.clamp(min=0).mean(dim=1)\n",
    "        R_text = R_text + torch.bmm(cam, R_text)\n",
    "    text_relevance = R_text\n",
    "   \n",
    "    return text_relevance, image_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9MJ-Ech7dj6C"
   },
   "outputs": [],
   "source": [
    "def show_image_relevance(image_relevance, image, orig_image):\n",
    "    # create heatmap from mask on image\n",
    "    def show_cam_on_image(img, mask):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        cam = heatmap + np.float32(img)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    axs[0].imshow(orig_image);\n",
    "    axs[0].axis('off');\n",
    "\n",
    "    dim = int(image_relevance.numel() ** 0.5)\n",
    "    image_relevance = image_relevance.reshape(1, 1, dim, dim)\n",
    "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
    "    image_relevance = image_relevance.reshape(224, 224).cuda().data.cpu().numpy()\n",
    "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
    "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    vis = show_cam_on_image(image, image_relevance)\n",
    "    vis = np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    axs[1].imshow(vis);\n",
    "    axs[1].axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NsNrdWXOxub1"
   },
   "outputs": [],
   "source": [
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def show_heatmap_on_text(text, text_encoding, R_text):\n",
    "  CLS_idx = text_encoding.argmax(dim=-1)\n",
    "  R_text = R_text[CLS_idx, 1:CLS_idx]\n",
    "  text_scores = R_text / R_text.sum()\n",
    "  text_scores = text_scores.flatten()\n",
    "  print(text_scores)\n",
    "  text_tokens=_tokenizer.encode(text)\n",
    "  text_tokens_decoded=[_tokenizer.decode([a]) for a in text_tokens]\n",
    "  vis_data_records = [visualization.VisualizationDataRecord(text_scores,0,0,0,0,0,text_tokens_decoded,1)]\n",
    "  visualization.visualize_text(vis_data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7YYjztv3Nn9V"
   },
   "outputs": [],
   "source": [
    "clip.clip._MODELS = {\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
    "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pns9sG9eAhho",
    "outputId": "180c0886-cb3f-4554-82e2-4a51250f6692"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7f45a7a92820>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4hIrp94ktMyc"
   },
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emphasize_attention(image_relevance, image):\n",
    "    # Create heatmap from attention mask\n",
    "    def create_heatmap(mask):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        return heatmap\n",
    "\n",
    "    # Apply attention-based augmentation\n",
    "    dim = int(image_relevance.numel() ** 0.5)\n",
    "    image_relevance = image_relevance.reshape(1, 1, dim, dim)\n",
    "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
    "    image_relevance = image_relevance.reshape(224, 224).cuda().data.cpu().numpy()\n",
    "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
    "\n",
    "\n",
    "    max_relevance = np.max(image_relevance)\n",
    "    max_indices = np.argwhere(image_relevance == max_relevance)\n",
    "    max_score = [(idx[1], idx[0]) for idx in max_indices]\n",
    "\n",
    "    unique_values, value_counts = np.unique(image_relevance, return_counts=True)\n",
    "    frequency_dict = dict(zip(unique_values, value_counts)) # Attention 값 빈도수\n",
    "\n",
    "    print(frequency_dict)\n",
    "\n",
    "    print(\"Pixel location of max score : \", max_score)\n",
    "\n",
    "    # Emphasize attention regions in the image\n",
    "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "    # Apply attention mask to image\n",
    "    heatmap = create_heatmap(image_relevance)\n",
    "    \n",
    "    emphasized_image = (heatmap * image) + ((1 - heatmap) * 255)\n",
    "    emphasized_image = np.uint8(emphasized_image)\n",
    "\n",
    "    return emphasized_image, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m\u001b[4mCLIP similarity score: 21.5625\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ResidualAttentionBlock' object has no attribute 'attn_probs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m logits_per_image, logits_per_text \u001b[39m=\u001b[39m model(img, text)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(color\u001b[39m.\u001b[39mBOLD \u001b[39m+\u001b[39m color\u001b[39m.\u001b[39mPURPLE \u001b[39m+\u001b[39m color\u001b[39m.\u001b[39mUNDERLINE \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCLIP similarity score: \u001b[39m\u001b[39m{\u001b[39;00mlogits_per_image\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m color\u001b[39m.\u001b[39mEND)\n\u001b[0;32m----> 9\u001b[0m R_text, R_image \u001b[39m=\u001b[39m interpret(model\u001b[39m=\u001b[39;49mmodel, image\u001b[39m=\u001b[39;49mimg, texts\u001b[39m=\u001b[39;49mtext, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     10\u001b[0m batch_size \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[1;32m     12\u001b[0m     \u001b[39m# Text heatmap\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m, in \u001b[0;36minterpret\u001b[0;34m(image, texts, model, device, start_layer, start_layer_text)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m start_layer \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m: \n\u001b[1;32m     16\u001b[0m   \u001b[39m# calculate index of last lareyer \u001b[39;00m\n\u001b[1;32m     17\u001b[0m   start_layer \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(image_attn_blocks) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 19\u001b[0m num_tokens \u001b[39m=\u001b[39m image_attn_blocks[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mattn_probs\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     20\u001b[0m R \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meye(num_tokens, num_tokens, dtype\u001b[39m=\u001b[39mimage_attn_blocks[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mattn_probs\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m R \u001b[39m=\u001b[39m R\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mexpand(batch_size, num_tokens, num_tokens)\n",
      "File \u001b[0;32m~/anaconda3/envs/jwnew2/lib/python3.9/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResidualAttentionBlock' object has no attribute 'attn_probs'"
     ]
    }
   ],
   "source": [
    "img_path = \"/home/iai/Desktop/Jeewon/Git/APE/cosmetic_images/19-69/Capri_Eau_de_Parfum/Capri_Eau_de_Parfum.jpg\"\n",
    "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "texts = [\"A bottle of a glossier\"]\n",
    "text = clip.tokenize(texts).to(device)\n",
    "\n",
    "logits_per_image, logits_per_text = model(img, text)\n",
    "print(color.BOLD + color.PURPLE + color.UNDERLINE + f'CLIP similarity score: {logits_per_image.item()}' + color.END)\n",
    "\n",
    "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
    "batch_size = text.shape[0]\n",
    "for i in range(batch_size):\n",
    "    # Text heatmap\n",
    "    show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
    "    \n",
    "    # Image heatmap\n",
    "    show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
    "    \n",
    "    # Emphasize attention regions in the image\n",
    "    _, max_score = emphasize_attention(R_image[i], img)\n",
    "    \n",
    "    # # Save the augmented image\n",
    "    # augmented_img_path = f\"augmented_image_{i}.jpg\"\n",
    "    # Image.fromarray(emphasized_image).save(augmented_img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m target_size \u001b[39m=\u001b[39m (\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\n\u001b[1;32m     38\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(image, target_size, interpolation\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mINTER_AREA)\n\u001b[0;32m---> 41\u001b[0m cropped_image \u001b[39m=\u001b[39m crop_object(image, max_score)\n\u001b[1;32m     43\u001b[0m cropped_img_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./cropped_image.jpg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m Image\u001b[39m.\u001b[39mfromarray(cropped_image)\u001b[39m.\u001b[39msave(cropped_img_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_score' is not defined"
     ]
    }
   ],
   "source": [
    "# Augmentation1 : crop 증강\n",
    "\n",
    "def crop_object(image, max_score):\n",
    "    # Randomly select a pixel from max_score\n",
    "    selected_pixel = random.choice(max_score)\n",
    "    \n",
    "\n",
    "    # Set the region size for cropping the object\n",
    "    min_region_size = int(0.30 * image.shape[0])  # 30% of image height\n",
    "    max_region_size = int(0.60 * image.shape[0])  # 60% of image height\n",
    "    \n",
    "\n",
    "    # Randomly generate the region size within the specified range\n",
    "    region_size = random.randint(min_region_size, max_region_size)\n",
    "\n",
    "    # Extract the coordinates of the selected pixel\n",
    "    x, y = selected_pixel\n",
    "\n",
    "    # Define the coordinates for the region to be cropped\n",
    "    x_start = max(0, x - region_size // 2)\n",
    "    x_end = min(image.shape[1], x + region_size // 2)\n",
    "    y_start = max(0, y - region_size // 2)\n",
    "    y_end = min(image.shape[0], y + region_size // 2)\n",
    "\n",
    "    # Crop the region from the image\n",
    "    cropped_image = image[y_start:y_end, x_start:x_end].copy()\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "# Inference\n",
    "\n",
    "image = cv2.imread(img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 이미지 크기 변경\n",
    "target_size = (224, 224)\n",
    "image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "cropped_image = crop_object(image, max_score)\n",
    "\n",
    "cropped_img_path = \"./cropped_image.jpg\"\n",
    "Image.fromarray(cropped_image).save(cropped_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Augmentation2 : highlight 증강\n",
    "\n",
    "def highlight_object(image, max_score):\n",
    "    # Randomly select a pixel from max_score\n",
    "    selected_pixel = random.choice(max_score)\n",
    "\n",
    "    # Set the region size for emphasizing the object\n",
    "    min_region_size = int(0.30 * image.shape[0])  # 30% of image height\n",
    "    max_region_size = int(0.60 * image.shape[0])  # 60% of image height\n",
    "    \n",
    "\n",
    "    # Randomly generate the region size within the specified range\n",
    "    region_size = random.randint(min_region_size, max_region_size)\n",
    "\n",
    "    # Extract the coordinates of the selected pixel\n",
    "    x, y = selected_pixel\n",
    "\n",
    "    # Define the coordinates for the region to be emphasized\n",
    "    x_start = max(0, x - region_size // 2)\n",
    "    x_end = min(image.shape[1], x + region_size // 2)\n",
    "    y_start = max(0, y - region_size // 2)\n",
    "    y_end = min(image.shape[0], y + region_size // 2)\n",
    "\n",
    "    # Apply blur to the non-emphasized region\n",
    "    emphasized_image = image.copy()\n",
    "    if y_start > 0:\n",
    "        emphasized_image[:y_start, :] = cv2.blur(emphasized_image[:y_start, :], (15, 15))\n",
    "    if y_end < image.shape[0]:\n",
    "        emphasized_image[y_end:, :] = cv2.blur(emphasized_image[y_end:, :], (15, 15))\n",
    "    if x_start > 0:\n",
    "        emphasized_image[:, :x_start] = cv2.blur(emphasized_image[:, :x_start], (15, 15))\n",
    "    if x_end < image.shape[1]:\n",
    "        emphasized_image[:, x_end:] = cv2.blur(emphasized_image[:, x_end:], (15, 15))\n",
    "\n",
    "    # Apply color/brightness/sharpness adjustment to the emphasized region\n",
    "    alpha = random.uniform(0.5, 2.0)  # Random scale factor for color adjustment\n",
    "    beta = random.randint(-50, 50)  # Random offset for brightness adjustment\n",
    "    gamma = random.uniform(0.5, 1.5)  # Random gamma factor for sharpness adjustment\n",
    "\n",
    "    # Apply color/brightness/sharpness adjustment to the emphasized region\n",
    "    emphasized_image[y_start:y_end, x_start:x_end] = cv2.convertScaleAbs(\n",
    "        emphasized_image[y_start:y_end, x_start:x_end], alpha=alpha, beta=beta)\n",
    "    emphasized_image[y_start:y_end, x_start:x_end] = cv2.addWeighted(\n",
    "        emphasized_image[y_start:y_end, x_start:x_end], gamma, emphasized_image[y_start:y_end, x_start:x_end], 0, 0)\n",
    "\n",
    "    return emphasized_image\n",
    "\n",
    "\n",
    "\n",
    "# Inference\n",
    "image = cv2.imread(img_path)\n",
    "# Convert BGR to RGB\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 이미지 크기 변경\n",
    "target_size = (224, 224)\n",
    "image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "highlighted_image = highlight_object(image, max_score)\n",
    "\n",
    "print(highlighted_image.shape)\n",
    "\n",
    "\n",
    "highlighted_img_path = \"./highlight_image.jpg\"\n",
    "Image.fromarray(highlighted_image).save(highlighted_img_path)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CLIP-explainability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
