{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchsummary import summary\n",
    "from typing import Callable, Dict, List, Tuple, Union\n",
    "from torch.optim import lr_scheduler\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# import skimage\n",
    "\n",
    "import clip #importing clip model\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available:\n",
    "  print('cuda available')\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\",jit=False) #loading the CLIP model based on ViT\n",
    "model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 2차 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  Clothes\n",
      "num of classes :  604\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6547047911469328\n",
      "Number of correct labels: 20470\n",
      "2  :  Electronic\n",
      "num of classes :  224\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6793798449612403\n",
      "Number of correct labels: 6573\n",
      "3  :  Food\n",
      "num of classes :  932\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6573758200562324\n",
      "Number of correct labels: 35071\n",
      "4  :  Leisure\n",
      "num of classes :  111\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.8441512752858399\n",
      "Number of correct labels: 4799\n",
      "5  :  Medical\n",
      "num of classes :  47\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.5802281368821293\n",
      "Number of correct labels: 2289\n",
      "6  :  Necessities\n",
      "num of classes :  432\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6200547901055515\n",
      "Number of correct labels: 15391\n",
      "7  :  Others\n",
      "num of classes :  371\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6337265519241926\n",
      "Number of correct labels: 9831\n",
      "8  :  Sports\n",
      "num of classes :  66\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6951682266632937\n",
      "Number of correct labels: 2748\n",
      "9  :  Transportation\n",
      "num of classes :  213\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6703685974150311\n",
      "Number of correct labels: 7002\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import os\n",
    "import clip\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\",jit=False) #loading the CLIP model based on ViT\n",
    "model.cuda().eval()\n",
    "\n",
    "folder=torchvision.datasets.ImageFolder(root='/workspace/classification_exp/dataset/LogoDet-3K') #커스텀데이터 부르기\n",
    "names = folder.classes\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(len(names)):\n",
    "    \n",
    "    folder_path = f\"/workspace/classification_exp/dataset/LogoDet-3K/{names[i]}\"\n",
    "    \n",
    "    print(i+1, \" : \", names[i])\n",
    "    \n",
    "    test_set = torchvision.datasets.ImageFolder(root=folder_path, transform=preprocess) #커스텀데이터 부르기\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False) #데이터를 미니배치 형태로 생성\n",
    "\n",
    "    print(\"num of classes : \", len(test_set.classes))\n",
    "\n",
    "    # 모델 및 텍스트 토큰화 함수 정의\n",
    "    text_tokens = clip.tokenize(test_set.classes).to(device)\n",
    "\n",
    "    # 모델 추론 및 정확도 계산\n",
    "    correct_labels = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 이미지 특성 추출\n",
    "            image_features = model.encode_image(images).float()\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # 텍스트 특성 추출\n",
    "            text_features = model.encode_text(text_tokens).float()\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # 텍스트 특성과의 유사도 계산\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            top_probs, top_labels = text_probs.topk(5, dim=-1)\n",
    "\n",
    "            # 정확도 계산\n",
    "            correct_labels += (top_labels[:, 0] == labels).sum().item()\n",
    "            total_labels += labels.size(0)\n",
    "\n",
    "    # 정확도 출력\n",
    "    accuracy = correct_labels / total_labels\n",
    "    print(\"The overall accuracy for the CLIP Zero-shot model without ensembling is: {}\".format(accuracy))\n",
    "    print(\"Number of correct labels:\", correct_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a photo of {label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  Clothes\n",
      "num of classes :  604\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6519541994498816\n",
      "Number of correct labels: 20384\n",
      "\n",
      "2  :  Electronic\n",
      "num of classes :  224\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6775193798449612\n",
      "Number of correct labels: 6555\n",
      "\n",
      "3  :  Food\n",
      "num of classes :  932\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.637282099343955\n",
      "Number of correct labels: 33999\n",
      "\n",
      "4  :  Leisure\n",
      "num of classes :  111\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.8402814423922603\n",
      "Number of correct labels: 4777\n",
      "\n",
      "5  :  Medical\n",
      "num of classes :  47\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.5077313054499366\n",
      "Number of correct labels: 2003\n",
      "\n",
      "6  :  Necessities\n",
      "num of classes :  432\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6198130690516477\n",
      "Number of correct labels: 15385\n",
      "\n",
      "7  :  Others\n",
      "num of classes :  371\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6486817507896603\n",
      "Number of correct labels: 10063\n",
      "\n",
      "8  :  Sports\n",
      "num of classes :  66\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6926385024032381\n",
      "Number of correct labels: 2738\n",
      "\n",
      "9  :  Transportation\n",
      "num of classes :  213\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.6667304930588799\n",
      "Number of correct labels: 6964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\",jit=False) #loading the CLIP model based on ViT\n",
    "model.cuda().eval()\n",
    "\n",
    "folder=torchvision.datasets.ImageFolder(root='/workspace/classification_exp/dataset/LogoDet-3K', transform=preprocess) #커스텀데이터 부르기\n",
    "names = folder.classes\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(len(names)):\n",
    "    \n",
    "    folder_path = f\"/workspace/classification_exp/dataset/LogoDet-3K/{names[i]}\"\n",
    "    \n",
    "    print(i+1, \" : \", names[i])\n",
    "    \n",
    "    test_set = torchvision.datasets.ImageFolder(root=folder_path, transform=preprocess) #커스텀데이터 부르기\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False) #데이터를 미니배치 형태로 생성\n",
    "\n",
    "    print(\"num of classes : \", len(test_set.classes))\n",
    "\n",
    "    # 모델 및 텍스트 토큰화 함수 정의\n",
    "    text_descriptions = [f\"a photo of a {label}\" for label in test_set.classes]\n",
    "    text_tokens = clip.tokenize(text_descriptions).to(device)\n",
    "\n",
    "    # 모델 추론 및 정확도 계산\n",
    "    correct_labels = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 이미지 특성 추출\n",
    "            image_features = model.encode_image(images).float()\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # 텍스트 특성 추출\n",
    "            text_features = model.encode_text(text_tokens).float()\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # 텍스트 특성과의 유사도 계산\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            top_probs, top_labels = text_probs.topk(5, dim=-1)\n",
    "\n",
    "            # 정확도 계산\n",
    "            correct_labels += (top_labels[:, 0] == labels).sum().item()\n",
    "            total_labels += labels.size(0)\n",
    "\n",
    "    # 정확도 출력\n",
    "    accuracy = correct_labels / total_labels\n",
    "    print(\"The overall accuracy for the CLIP Zero-shot model with ensembling is: {}\".format(accuracy))\n",
    "    print(\"Number of correct labels:\", correct_labels)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 1차 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data instance is :  158654\n"
     ]
    }
   ],
   "source": [
    "total_num = 0\n",
    "\n",
    "folder=torchvision.datasets.ImageFolder(root='/workspace/classification_exp/dataset/LogoDet-3K', transform=preprocess) #커스텀데이터 부르기\n",
    "names = folder.classes\n",
    "\n",
    "for i in range(len(names)):\n",
    "    folder_path = f\"/workspace/classification_exp/dataset/LogoDet-3K/{names[i]}\"\n",
    "    dataset = torchvision.datasets.ImageFolder(root=folder_path, transform=preprocess) #커스텀데이터 부르기\n",
    "    total_num += len(dataset)\n",
    "    \n",
    "    \n",
    "print(\"Total number of data instance is : \", total_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of classes :  3000\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.11409734390560591\n",
      "Number of correct labels: 18102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "import clip\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\",jit=False) #loading the CLIP model based on ViT\n",
    "model.cuda().eval()\n",
    "\n",
    "folder=torchvision.datasets.ImageFolder(root='/workspace/classification_exp/dataset/LogoDet-3K', transform=preprocess) #커스텀데이터 부르기\n",
    "names = folder.classes\n",
    "class_names = []\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "folder_path = f\"/workspace/classification_exp/dataset/LogoDet-3K/{names[0]}\"\n",
    "combined_dataset = torchvision.datasets.ImageFolder(root=folder_path, transform=preprocess) #커스텀데이터 부르기\n",
    "class_names+=(combined_dataset.classes)\n",
    "\n",
    "for i in range(len(names)-1):\n",
    "    folder_path = f\"/workspace/classification_exp/dataset/LogoDet-3K/{names[i+1]}\"\n",
    "    dataset = torchvision.datasets.ImageFolder(root=folder_path, transform=preprocess) #커스텀데이터 부르기\n",
    "    combined_dataset = ConcatDataset([combined_dataset, dataset])\n",
    "    \n",
    "    class_names+=(dataset.classes)\n",
    "\n",
    "print(\"num of classes : \", len(class_names))\n",
    "    \n",
    "test_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=False) #데이터를 미니배치 형태로 생성\n",
    "\n",
    "# 모델 및 텍스트 토큰화 함수 정의\n",
    "text_tokens = clip.tokenize(class_names).to(device)\n",
    "\n",
    "# 모델 추론 및 정확도 계산\n",
    "correct_labels = 0\n",
    "total_labels = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 이미지 특성 추출\n",
    "        image_features = model.encode_image(images).float()\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # 텍스트 특성 추출\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # 텍스트 특성과의 유사도 계산\n",
    "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        top_probs, top_labels = text_probs.topk(5, dim=-1)\n",
    "\n",
    "        # 정확도 계산\n",
    "        correct_labels += (top_labels[:, 0] == labels).sum().item()\n",
    "        total_labels += labels.size(0)\n",
    "\n",
    "# 정확도 출력\n",
    "accuracy = correct_labels / total_labels\n",
    "print(\"The overall accuracy for the CLIP Zero-shot model without ensembling is: {}\".format(accuracy))\n",
    "print(\"Number of correct labels:\", correct_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a photo of {label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of classes :  3000\n",
      "The overall accuracy for the CLIP Zero-shot model without ensembling is: 0.11472134330051559\n",
      "Number of correct labels: 18201\n"
     ]
    }
   ],
   "source": [
    "print(\"num of classes : \", len(class_names))\n",
    "    \n",
    "test_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=False) #데이터를 미니배치 형태로 생성\n",
    "\n",
    "# 모델 및 텍스트 토큰화 함수 정의\n",
    "text_descriptions = [f\"a photo of a {label}\" for label in class_names]\n",
    "text_tokens = clip.tokenize(text_descriptions).to(device)\n",
    "\n",
    "# 모델 추론 및 정확도 계산\n",
    "correct_labels = 0\n",
    "total_labels = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 이미지 특성 추출\n",
    "        image_features = model.encode_image(images).float()\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # 텍스트 특성 추출\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # 텍스트 특성과의 유사도 계산\n",
    "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        top_probs, top_labels = text_probs.topk(5, dim=-1)\n",
    "\n",
    "        # 정확도 계산\n",
    "        correct_labels += (top_labels[:, 0] == labels).sum().item()\n",
    "        total_labels += labels.size(0)\n",
    "\n",
    "# 정확도 출력\n",
    "accuracy = correct_labels / total_labels\n",
    "print(\"The overall accuracy for the CLIP Zero-shot model with ensembling is: {}\".format(accuracy))\n",
    "print(\"Number of correct labels:\", correct_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
